# two player training multiprocessed

import random
import numpy as np
import torch
import multiprocessing
import os
import matplotlib.pyplot as plt
import pandas as pd
from agents.agents_ai import RLAgent
from helper import swap_halves_tensor
from helper_plotting import plot_stats
from two_player_game import two_player_game
from helper import training_plan

def run_game_for_experience(board_size, max_turn, game_idx, p0='RLAgent', p1='random', output_dir='./output'):
    """
    Runs a single game simulation and extracts relevant training data.
    """
    try:
        result = two_player_game(
            board_size=board_size,
            max_turn=max_turn,
            p0=p0,
            p1=p1,
            do_plot=False,
            do_update_agent=False,
            do_train=True,
            do_save=False,
            game_idx=game_idx
        )
    except Exception as e:
        print(f"[ERROR] Game {game_idx} failed: {e}")
        return None
    return result

def collect_experiences_parallel(batch_start_idx, N_games, board_size, max_turn, p0='RLAgent', p1='random', output_dir='./output'):
    """
    Collects gameplay data in parallel using multiprocessing.
    """
    args = [(board_size, max_turn, batch_start_idx + i, p0, p1, output_dir) for i in range(N_games)]
    ctx = multiprocessing.get_context('spawn')
    with ctx.Pool() as pool:
        results = pool.starmap(run_game_for_experience, args)
    return [r for r in results if r is not None]

def centralized_training(N_games, board_size, max_turn, p0='RLAgent', p1='random', N_CORES=4, output_dir='./output',start_from_scratch=True):
    # Clean output directory
    files_dir = os.path.join(output_dir, 'files')
    figs_dir = os.path.join(output_dir, 'fig')
    if os.path.exists(files_dir):
        for file in os.listdir(files_dir):
            os.remove(os.path.join(files_dir, file))
    if os.path.exists(figs_dir):
        for file in os.listdir(figs_dir):
            os.remove(os.path.join(figs_dir, file))
    """
    Runs centralized training for multiple games, computes discounted rewards,
    trains the RL agent, and plots summary statistics.
    """
    param_dict = training_plan(0)
    gamma = param_dict['gamma']
    epsilon = param_dict['epsilon']
    lr = param_dict['learning_rate']

    agent = RLAgent(epsilon=epsilon, learning_rate=lr)

    results = {
        "final_scores": [],
        "final_cumulative_rewards": [],
        "total_cumulative_rewards": [],
        "losses": []
    }

    game_idx = 0
    if not start_from_scratch: game_idx += 1

    while game_idx < N_games:
        remaining_games = N_games - game_idx
        batch_size = min(N_CORES, remaining_games)
        print(f"[INFO] Batch progress: game {game_idx}/{N_games} ({(game_idx/N_games)*100:.1f}%) with {N_CORES} workers")

        game_data_batch = collect_experiences_parallel(game_idx, batch_size, board_size, max_turn, p0, p1, output_dir)

        for batch_idx, data in enumerate(game_data_batch):
            actual_game_idx = game_idx + batch_idx
            print(f"[INFO] Training on game {actual_game_idx} (core {batch_idx+1}/{len(game_data_batch)})")

            # Unpack tuple to dictionary with keys
            data = {
                "player_labels": data[0],
                "score_history": data[1],
                "rewards_history": data[2],
                "rewards_cumul_history": data[3],
                "A": data[4],
                "B": data[5],
                "C": data[6],
                "player0_agent": data[7],
                "player1_agent": data[8],
                "action_vector_dict": data[9]
            }

            if data is None or data["action_vector_dict"] is None:
                continue

            action_vector_dict = data["action_vector_dict"]
            reward_history = data["rewards_history"]
            score_history = data["score_history"]

            T = len(action_vector_dict)
            rewards_cumul = [0, 0]
            cumul_hist = []

            for t in reversed(range(T)):
                rewards_cumul[0] = reward_history[t][0] + gamma * rewards_cumul[0]
                rewards_cumul[1] = reward_history[t][1] + gamma * rewards_cumul[1]
                cumul_hist.insert(0, rewards_cumul.copy())
            loss_hist = []
            
            # create a list of indices and shuffle it
            shuffled_indices = list(range(T))
            random.shuffle(shuffled_indices)

            # train using the shuffled index order
            for t in shuffled_indices:                
                board_tensor = action_vector_dict[t]
                reward_cumul = cumul_hist[t]
                loss0, _, _ = agent.train_step(board_tensor, reward_cumul[0], 0)
                board_tensor_inverted = swap_halves_tensor(board_tensor)
                loss1, _, _ = agent.train_step(board_tensor_inverted, reward_cumul[1], 1)
                loss_hist.append([loss0, loss1])

            results["final_scores"].append(score_history[-1])
            results["final_cumulative_rewards"].append(cumul_hist[-1])
            results["total_cumulative_rewards"].append(np.sum(cumul_hist,axis=0))
            results["losses"].append(np.sum(loss_hist, axis=0))

        labels = data["player_labels"]
        plot_stats(output_dir, labels, results["final_scores"], 'Final Scores Per Game', 'Score', 'final_scores_stats.png',N_CORES=N_CORES)
        plot_stats(output_dir, labels, results["final_cumulative_rewards"], 'Final Cumulative Rewards Per Game', 'Cumulative Reward', 'final_cumul_rewards_stats.png',N_CORES=N_CORES)
        plot_stats(output_dir, labels, results["total_cumulative_rewards"], 'Total Cumulative Rewards Per Game', 'Cumulative Reward', 'total_cumul_rewards_stats.png',N_CORES=N_CORES)
        plot_stats(output_dir, labels, results["losses"], 'Total Losses Per Game', 'Loss', 'losses_stats.png',N_CORES=N_CORES)

        agent.save_model()  # Save model after every episode batch
        game_idx += batch_size
        print(f"[INFO] Completed {game_idx}/{N_games} games ({(game_idx/N_games)*100:.1f}%)")
            
    agent.save_model()  # Final model save

    # Re-load simplified CSV for consistency

if __name__ == '__main__':
    multiprocessing.set_start_method('spawn', force=True)
    random.seed(0)
    np.random.seed(0)

    centralized_training(
        N_games=1024,
        board_size=15,
        max_turn=500,
        p0='RLAgent',
        p1='score_max_own',
        N_CORES=8,
        output_dir='./output',
        start_from_scratch=False
    )
